import runpod
import os
import subprocess
import requests
import torch
import numpy
import onnxruntime

# --- –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê ---
print(f"NumPy –≤–µ—Ä—Å–∏—è: {numpy.__version__}")
print(f"ONNX Runtime –≤–µ—Ä—Å–∏—è: {onnxruntime.__version__}")
print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: {onnxruntime.get_available_providers()}")
print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
print("=" * 60)

def download_file(url, save_path):
    if os.path.exists(save_path):
        return save_path
    print(f"üì• –°–∫–∞—á–∏–≤–∞—é —Ñ–∞–π–ª: {url}")
    response = requests.get(url, stream=True)
    response.raise_for_status()
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
    return save_path

def handler(job):
    job_input = job['input']
    source_url = job_input.get('source_url')
    target_url = job_input.get('target_url')

    if not source_url or not target_url:
        return {"error": "–ù—É–∂–Ω—ã source_url –∏ target_url"}

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
    os.makedirs('/tmp/input', exist_ok=True)
    os.makedirs('/tmp/output', exist_ok=True)

    source_path = "/tmp/input/source.jpg"
    target_path = "/tmp/input/target.mp4"
    output_path = "/tmp/output/result.mp4"

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    download_file(source_url, source_path)
    download_file(target_url, target_path)

    # --- –ù–ê–°–¢–†–û–ô–ö–ê –ö–û–ú–ê–ù–î–´ –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –°–ö–û–†–û–°–¢–ò ---
    # –î–ª—è RTX 4090 –º—ã –º–æ–∂–µ–º —Å–º–µ–ª–æ —Å—Ç–∞–≤–∏—Ç—å –º–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–æ–≤ (execution-thread-count)
    # –ò –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é 'high' –∏–ª–∏ 'relaxed' –¥–ª—è –ø–∞–º—è—Ç–∏.
    command = [
        "python", "facefusion.py",
        "headless-run",
        "--execution-providers", "cuda",
        "--processors", "face_swapper",
        "--execution-thread-count", "24",  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è 4090
        "--execution-queue-count", "2",    # –û—á–µ—Ä–µ–¥—å –∫–∞–¥—Ä–æ–≤
        "--video-memory-strategy", "high", # –ü–æ–∑–≤–æ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—é VRAM
        "--skip-download",                 # –ú—ã –≤—Å—ë —Å–∫–∞—á–∞–ª–∏ –≤ Docker
        "-s", source_path,
        "-t", target_path,
        "-o", output_path
    ]

    print(f"üîß –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò (GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ)...")
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∏ –ª–æ–≤–∏–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        for line in process.stdout:
            print(line, end='')
        
        process.wait()

        if os.path.exists(output_path):
            print("‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç–ø—Ä–∞–≤–ª—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–≤–æ—è –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ S3 –∏–ª–∏ Telegram)
            return {"status": "success", "message": "Video processed"}
        else:
            return {"status": "error", "message": "Output file not found"}

    except Exception as e:
        return {"status": "error", "message": str(e)}

runpod.serverless.start({"handler": handler})